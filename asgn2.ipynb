{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9fd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"maximedb/natural_questions\")\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"sentence-transformers/natural-questions\")\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"JasleenSingh91/travel-questions-response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "# from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "# from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "# import nltk\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# # Download commonly used NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('treebank', download_dir='/mnt/data/treebank')\n",
    "# nltk.data.path.append('/mnt/data/treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Acquire the training data from Hugging Face\n",
    "ds = load_dataset(\"maximedb/natural_questions\", trust_remote_code=True)\n",
    "# ds = load_dataset(\"JasleenSingh91/travel-questions-response\")\n",
    "\n",
    "# ds = ds[\"train\"].train_test_split(test_size= 0.9)\n",
    "ds = ds[\"train\"].train_test_split(test_size= 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e633a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 104186\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 26047\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bbd2c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer, model, and data collator\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)  \n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419df92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 823\n"
     ]
    }
   ],
   "source": [
    "max_len_src = 0\n",
    "max_len_tgt = 0\n",
    "\n",
    "for item in ds[\"train\"]:\n",
    "    # print(item['translation'][source_lang])\n",
    "    # break\n",
    "    src_ids = tokenizer.encode(item['question'])\n",
    "    tgt_ids = tokenizer.encode(item['answer'])\n",
    "    max_len_src = max(max_len_src, len(src_ids))\n",
    "    max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "    \n",
    "print(max_len_src,max_len_tgt)\n",
    "\n",
    "dataset_en = \"\"\n",
    "for item in raw_datasets[\"train\"]:\n",
    "    dataset_en += \" \" + item['translation'][source_lang]\n",
    "word_cloud = WordCloud().generate(dataset_en)\n",
    "plt.imshow(word_cloud)\n",
    "plt.show()\n",
    "\n",
    "dataset_fi = \"\"\n",
    "for item in raw_datasets[\"train\"]:\n",
    "    dataset_fi += \" \" + item['translation'][target_lang]\n",
    "word_cloud = WordCloud().generate(dataset_fi)\n",
    "plt.imshow(word_cloud)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97a8c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We prefix our tasks with \"answer the question\"\n",
    "prefix = \"Answer this question: \"\n",
    "\n",
    "# Define the preprocessing function\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Add prefix to the sentences, tokenize the text, and set the labels\"\"\"\n",
    "    # The \"inputs\" are the tokenized answer:\n",
    "    inputs = [prefix + doc for doc in examples[\"question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    # The \"labels\" are the tokenized outputs:\n",
    "    labels = tokenizer(text_target=examples[\"answer\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bad3dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 104186/104186 [00:04<00:00, 24076.51 examples/s]\n",
      "Map: 100%|██████████| 26047/26047 [00:01<00:00, 23795.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Map the preprocessing function across our dataset\n",
    "tokenized_dataset = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b577654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pingj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\pingj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\pingj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Set up Rouge score for evaluation\n",
    "# nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")\n",
    "meteor_score = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def simple_sent_tokenize(text):\n",
    "    # A naive sentence tokenizer using regex\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return [s for s in sentences if s]\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Replace -100 with pad_token_id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "\n",
    "    # Decode token IDs to text\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Use naive sentence tokenizer instead of nltk\n",
    "    decoded_preds = [\"\\n\".join(simple_sent_tokenize(pred)) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(simple_sent_tokenize(label)) for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    meteor_res = meteor_score.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # precision, recall, f1, _ = precision_recall_fscore_support(decoded_labels, decoded_preds, average=\"weighted\")\n",
    "\n",
    "    return {\n",
    "            'rouge':result,\n",
    "            'meteor':meteor_res,\n",
    "            # 'precision':precision, \n",
    "            # 'recall': recall,\n",
    "            # 'f1': f1,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943710f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=10, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    # task_type=\"SEQ_2_SEQ_LM\" # FLAN-T5\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abbef83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 4\n",
    "# PER_DEVICE_EVAL_BATCH = 2\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 3\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=L_RATE,\n",
    "    # per_device_train_batch_size=BATCH_SIZE,\n",
    "    # per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    save_total_limit=SAVE_TOTAL_LIM,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    predict_with_generate=True,\n",
    "    push_to_hub=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pingj\\AppData\\Local\\Temp\\ipykernel_17700\\3181277492.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Set up trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3bc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39072' max='39072' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39072/39072 4:28:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge</th>\n",
       "      <th>Meteor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.038800</td>\n",
       "      <td>2.808671</td>\n",
       "      <td>{'rouge1': 0.09065554278515837, 'rouge2': 0.020910035534594227, 'rougeL': 0.08894574429163021, 'rougeLsum': 0.08908224130594647}</td>\n",
       "      <td>{'meteor': 0.06441917677899289}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.196300</td>\n",
       "      <td>2.788157</td>\n",
       "      <td>{'rouge1': 0.09110314795688518, 'rouge2': 0.021867766849236785, 'rougeL': 0.08942928027198242, 'rougeLsum': 0.08960741987696558}</td>\n",
       "      <td>{'meteor': 0.06495903525554858}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.155300</td>\n",
       "      <td>2.776651</td>\n",
       "      <td>{'rouge1': 0.09366268502913269, 'rouge2': 0.023107689932829748, 'rougeL': 0.09188865313704514, 'rougeLsum': 0.09201423875098197}</td>\n",
       "      <td>{'meteor': 0.06707462212578694}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39072, training_loss=3.139088595827567, metrics={'train_runtime': 16121.4385, 'train_samples_per_second': 19.388, 'train_steps_per_second': 2.424, 'total_flos': 9040485033768960.0, 'train_loss': 3.139088595827567, 'epoch': 3.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger the model trainingnatural 512 token length\n",
    "# trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41217e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "last_checkpoint = \"./results/checkpoint-39072\"\n",
    "\n",
    "# finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(last_checkpoint)\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(last_checkpoint)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4623746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_question = \"what is the disadvantage of AI?\"\n",
    "inputs = \"Answer this question: \" + my_question\n",
    "\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5507f639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: <pad> a lack of a human 's intelligence</s>\n"
     ]
    }
   ],
   "source": [
    "outputs = finetuned_model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9650aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: <pad> Beijing</s>\n"
     ]
    }
   ],
   "source": [
    "my_question = \"what is the capital of China?\"\n",
    "inputs = \"Answer this question: \" + my_question\n",
    "\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96afc617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: <pad> England</s>\n"
     ]
    }
   ],
   "source": [
    "my_question = \"where is UK?\"\n",
    "inputs = \"Answer this question: \" + my_question\n",
    "\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d34cc3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: <pad> Superman</s>\n"
     ]
    }
   ],
   "source": [
    "my_question = \"Who is the most powerful superhero?\"\n",
    "inputs = \"Answer this question: \" + my_question\n",
    "\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c31b07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: <pad> YUGBHKL</s>\n"
     ]
    }
   ],
   "source": [
    "my_question = \"YUGBHKL\"\n",
    "inputs = \"Answer this question: \" + my_question\n",
    "\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f748f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "last_checkpoint = \"./checkpoint-1803\"\n",
    "\n",
    "# finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(last_checkpoint)\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(last_checkpoint)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7949ca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: <pad> Absolutely! I'd be happy to help you plan a trip to the UK. Here are\n"
     ]
    }
   ],
   "source": [
    "my_question = \"travel plan for UK\"\n",
    "inputs = \"Answer this question: \" + my_question\n",
    "\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e19068e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: <pad> Mary</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "my_question = \"who is the mother in the how i met your mother?\"\n",
    "inputs = \"Answer this question: \" + my_question\n",
    "\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "answer = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61487c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af88380e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9719218e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.52.4', '1.8.1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "transformers.__version__, accelerate.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
